import os
from dotenv import load_dotenv
from langchain.chains import RetrievalQA
from langchain_openai import ChatOpenAI
from langchain_openai import OpenAIEmbeddings
from langchain_qdrant import QdrantVectorStore
from langchain.prompts import PromptTemplate
from qdrant_client import QdrantClient

load_dotenv()
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")

qdrant = QdrantClient(host="localhost", port=6333)

embedding = OpenAIEmbeddings(model="text-embedding-3-small")

collection_name = "ktm-faq"
vectorstore = QdrantVectorStore(
    client=qdrant,
    collection_name=collection_name,
    embedding=embedding
)

llm = ChatOpenAI(model="gpt-3.5-turbo")
custom_prompt = PromptTemplate.from_template("""
ë‹¹ì‹ ì€ KT ê³ ê° FAQ ë¹„ì„œì…ë‹ˆë‹¤.
ì•„ë˜ì˜ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€ë‹µí•˜ì„¸ìš”.

ë§Œì•½ ê´€ë ¨ëœ ì •ë³´ê°€ ë¬¸ì„œ ì•ˆì— ì „í˜€ ì—†ê±°ë‚˜ ë¶ˆí™•ì‹¤í•˜ë‹¤ë©´,
ë°˜ë“œì‹œ ë‹¤ìŒê³¼ ê°™ì´ ë‹µë³€í•˜ì„¸ìš”:

"ì•ˆë…•í•˜ì„¸ìš”! ì§ˆë¬¸ì„ ë‹¤ì‹œ í•´ì£¼ì„¸ìš”."

=========
{context}
=========
ì§ˆë¬¸: {question}
ë‹µë³€:
""")

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever(search_kwargs={"k": 2}),
    return_source_documents=True,
    chain_type_kwargs={"prompt": custom_prompt}
)


question = "ì™¸ê³„ì¸ì´ ë˜ê³ ì‹¶ì–´"

result = qa_chain.invoke({"query": question})

print("\nğŸŸ¢ ë‹µë³€:")
print(result["result"])